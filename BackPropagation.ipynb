{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BackPropagation.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPRyk4ClNpbfEBL8zQOGP/t",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Laba-lab/myNeuralNetwork/blob/main/BackPropagation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_TSBi02hV7Xf"
      },
      "source": [
        "# Trying to use input information from Building_AI Exercise 21 and\n",
        "# adopt to Backpropagation example of NN from https://dev.to/shamdasani/buiild-a-flexible.... \n",
        "\n",
        "# Exercise 21 network consits of 5 input nodes, a hidden layer with two nodes, \n",
        "# second hidden layer with two nodes and finally output node \n",
        "\n",
        "import numpy as np\n",
        "\n",
        "X = np.array([  [111, 13, 12, 1, 161],\n",
        "                [125, 13, 66, 1, 468],\n",
        "                [46, 6, 127, 2, 962],\n",
        "                [80, 9, 80, 2, 816],\n",
        "                [33, 10, 18, 2, 297],\n",
        "                [85, 9, 111, 3, 601],\n",
        "                [24, 10, 105, 2, 1072],\n",
        "                [31, 4, 66, 1, 417],\n",
        "                [56, 3, 60, 1, 36],\n",
        "                [49, 3, 147, 2, 179] ], dtype=float)\n",
        "                \n",
        "x_test = np.array([ [82, 2, 65, 3, 516],\n",
        "                    [72, 2, 25, 3, 450],\n",
        "                    [60, 3, 15, 1, 300], \n",
        "                    [74, 5, 10, 2, 100] ], dtype=float)\n",
        "\n",
        "y = np.array([[335800.0], [379100.0], [118950.0], [247200.0], [107950.0], [26550.0], [75850.0], [93300.0], [170650.0], [149000.0]], dtype=float)\n",
        "\n",
        "# bias nodes (as Term Intercept 'a' in Linear regression))\n",
        "b0 = np.array([-4.21310294, -0.52664488])\n",
        "b1 = np.array([-4.84067881, -4.53335139])\n",
        "b2 = np.array([-7.52942418])\n",
        "\n",
        "\n",
        "# SCALE UNITS\n",
        "# We want to normalize units as our inputs are in hours, but our output is a test score from 0-100.\n",
        "# Therefore, we need to scale our data by dividing by the maximum value for each variable\n",
        "X = X/np.amax(X, axis=0)  # maximum of X array\n",
        "y = y/1000000.0                # maximum price of the cabin is 379100\n",
        "b0 = b0/7.52942418\n",
        "b1 = b1/7.52942418\n",
        "b2 = b2/7.52942418\n",
        "\n",
        "# DEFINE a python \"class\" and write an \"init\" function where we'll specify our parameters such as \n",
        "# input, hidden, and output layers\n",
        "class Neural_Network(object):\n",
        "    def __init__(self):\n",
        "        #parameters\n",
        "        self.inputSize = 5    # 5 input nodes\n",
        "        self.outputSize = 1   # one outut node\n",
        "        self.hidden1Size = 2   # 2 nodes in first hidden layer\n",
        "        self.hidden2Size = 2   # 2 hidden nodes in second hidden layer\n",
        "        \n",
        "        # GENERATE INITIAL WEIGHTS RANDOMLY\n",
        "        # We need 3 sets of weights, one to go from the input to the 1st hidden layer,\n",
        "        # another to go from 1st hidden layer to 2nd hidden layer,\n",
        "        # and other set of weights to go from the 2nd hidden layer to output layer\n",
        "        self.W0 = np.random.randn(self.inputSize, self.hidden1Size)  # (5x2) weights - Five input nodes to two hidden nodes\n",
        "        self.W1 = np.random.randn(self.hidden1Size, self.hidden2Size)   # (2x2) weights - 2 hidden to 2 2nd hidden\n",
        "        self.W2 = np.random.randn(self.hidden2Size, self.outputSize) # (3x1) weights\n",
        "        \n",
        "        \n",
        "    # **** FORWARD PROPAGATION FUNCTION ****\n",
        "    # Let's pass in our input X and use variable z to simulate the activity between the input and output layers\n",
        "    # We need to take a dot product (martix multiplication) of the inputs and weights,\n",
        "    # apply an activation function, take another dot product of the hidden layer and \n",
        "    # another set ow weights, and lastly apply a final activation function to recive the output.\n",
        "    def forward(self, X):\n",
        "        # forward propagation through our network\n",
        "        self.z0 = np.dot(X, self.W0)+b0    # dot product of X (input) and first set of weights\n",
        "        self.z1 = self.sigmoid(self.z0)    # activation function gives output from first hidden layer\n",
        "        \n",
        "        self.z2 = np.dot(self.z1, self.W1)+b1    # dot product of 1st hidden layer becomes input to 2nd hidden layer\n",
        "        self.z3 = self.sigmoid(self.z2)          # activation function gives output from second hidden layer\n",
        "        \n",
        "        self.z4 = np.dot(self.z3, self.W2)+b2   # dot product of 2nd hidden layer and weights W2 gives input to output node\n",
        "        o = self.sigmoid(self.z4)               # activation function for output\n",
        "        \n",
        "        return o\n",
        "        \n",
        "        \n",
        "    # DEFINE A SIGMOID FUNCTION\n",
        "    def sigmoid(self, s):\n",
        "        return 1/(1+np.exp(-s))\n",
        "        \n",
        "     \n",
        "    # DEFINE SIGMOID PRIME - DERIVATIVE OF SIGMOID FUNCTION    \n",
        "    def sigmoidPrime(self, s):\n",
        "        # derivative of sigmoid\n",
        "        return s*(1-s)\n",
        "        \n",
        "        \n",
        "    # **** DEFINE A BACKWARD PROPAGATION FUNCTION **** \n",
        "    # That does everything specified in four steps of \n",
        "    # calculating the incremental change to our weights:\n",
        "    def backward(self, X, y, o):\n",
        "        # backward propagate through the network\n",
        "        self.o_error = y - o \n",
        "        #print(\"This is self.o_error = y - o\", self.o_error)                             # error in output (STEP#1)\n",
        "        self.o_delta = self.o_error*self.sigmoidPrime(o)  # apply derivative of sigmoid to error (STEP#2)\n",
        "        #print(\" This is self.o_delta:\", self.o_delta)\n",
        "        \n",
        "        self.z3_error = self.o_delta.dot(self.W2.T)  # z3 error: how much our second hidden layer weights contributed to output error\n",
        "        #print(\"This is self.z3_error:\", self.z3_error)\n",
        "        self.z3_delta = self.z3_error*self.sigmoidPrime(self.z3)  # applying derivative of sigmoid to z2 error\n",
        "        #print(\" This is self.z3_delta:\", self.z3_delta)\n",
        "        \n",
        "        self.z1_error = self.z3_delta.dot(self.W1.T)  # z1 error: how much our first hidden layer weights contributed to output error\n",
        "        #print(\"This is self.z1_error:\", self.z1_error)\n",
        "        self.z1_delta = self.z1_error*self.sigmoidPrime(self.z1)  # applying derivative of sigmoid to z1 error\n",
        "        #print(\"This is self.z1_delta:\", self.z1_delta)\n",
        "        \n",
        "        #self.z3_error = self.o_delta.dot(self.W2.T)  # z2 error: how much our second hidden layer weights contributed to output error\n",
        "        #self.z3_delta = self.z3_error*self.sigmoidPrime(self.z3)  # applying derivative of sigmoid to z2 error\n",
        "        \n",
        "        self.W0 += X.T.dot(self.z1_delta)      # adjusting first set (input --> hidden) weights\n",
        "        self.W1 += self.z1.T.dot(self.z3_delta)  # adjusting second set of weights hidde1 --> hidden2\n",
        "        self.W2 += self.z3.T.dot(self.o_delta)   # adjusting second set (hidden2 --> output) weights\n",
        "   \n",
        "    \n",
        "    # DEFINE TRAIN FUNCTION\n",
        "    # We can now define our output through initiating forward propagation and initiate \n",
        "    # the backward function by calling it in the \"Train\" function\n",
        "    def train(self, X, y):\n",
        "        o = self.forward(X)\n",
        "        self.backward(X, y, o)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V4D7O8k2YHTv"
      },
      "source": [
        "# To run the network, all we have to do is to run the \"train\" function. Of course, we'll want to do this multiple times, \n",
        "# or maybe thousand of times. So we'll use a for loop          \n",
        "NN = Neural_Network()\n",
        "\n",
        "for i in range(10000):\n",
        "    print(\"Input: \\n\" + str(X))\n",
        "    print(\"Actual output: \\n\" + str(y))\n",
        "    print(\"Predicted output: \\n\" + str(NN.forward(X)))\n",
        "    print(\"Loss: \\n\" + str(np.mean(np.square(y-NN.forward(X)))))\n",
        "    print(\"\\n\")\n",
        "    NN.train(X, y)\n",
        "    \n",
        "    \n",
        "# TEST: 10 000 iteracija\n",
        "# Raspberry3 2 min in 37 sekunda = 157 000 miliseconds\n",
        "# 15.7 ms po iteraciji\n",
        "\n",
        "# TEST2: 10 000 iteracija\n",
        "# Included bias node values\n",
        "# 2 min 37 s\n",
        "\n",
        "# TEST3: 10 000 iteracija\n",
        "# Raspberry 4. Included bias node values\n",
        "# 45 s. 4.5 ms po iteraciji\n",
        "\n",
        "# TEST4: 10 000 iteracija\n",
        "# Google Colab Server\n",
        "# 44 s, 4.32 ms po iteraciji"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}